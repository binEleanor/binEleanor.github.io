
---
layout: post
title:  "浅析交叉熵损失函数"
date:   2019-08-11
excerpt: "介绍交叉熵损失函数，包括sigmoid交叉熵损失函数的推导"
tag:
- 交叉熵损失函数
comments: true
---

## 浅析交叉熵损失函数

首先抛出交叉熵损失函数分别用于二分类和多分类的损失函数表达式子：

**二分类交叉熵损失函数**

$$
L = -[ylogp +(1-y)log(1-p)]
$$
其中$$y$$表示样本标签，$$p$$表示对应样本标签预测为正的概率，如当



$$
\begin{equation}  
L = \left\{  
             \begin{array}{**lr**} 
             -log p,  & y = 1  \\  
             -log(1-p), & y = 0\\  
             \end{array}  
\right.  
\end{equation}
$$

**多分类交叉熵损失函数**

$$
L = -\sum_{c=1}^{M}y_clogp_c
$$

$$p_c$$表示标签预测为$$c$$的概率



### 最大似然估计

通过最大似然估计探测一下交叉熵损失函数的本质：  

最大似然估计的计算表达式为：  

$$
p(y|x) = \prod_{i=1}^{N}p_i^{y_i}(1-p_i)^{(1-y_i)}
$$

一般通过对数似然的方法对上述式子求解：

$$
\begin{align}
logp(y|x) & = \sum_{i=1}^N(y_ilogp_i + (1-y_i)log(1-p_i)) \\
          & = ylogp + (1-y)log(1-p)
\end{align}
$$

我们的目的是使得似然函数最大，即对于负的对数似然函数最小，仔细瞅一瞅，不就是我们的交叉熵损失函数了吗？



tensorflow中关于交叉熵损失函数的计算函数：

1. tf.nn.softmax_cross_entropy_with_logits(labels=input_labels, logits=pred_logits)

   要求输入的标签input_labels与计算出的pred_logits的维度一致；

2. tf.nn.sparse_softmax_cross_entropy_with_logits(labels=input_labels, logits=pred_logits)

   输入的标签是具体哪个标签的值，就是不需要进行one_hot等等编码的标签。

3. tf.nn.softmax_cross_entropy_with_logits_v2(labels=input_labels, logits=pred_logits)

   这个函数是tensorflow最新版本的计算交叉熵损失函数的函数接口，主要变化在于其对于输入标签不一定要求是保持不变的，如在对抗生成网络的训练过程中，标签可能是动态变化的，如果我们还是基于不变的标签，可以把这个函数当作跟第一个函数一样来使用。



### sigmoid交叉熵损失函数

Sigmoid的概率计算公式为：  
$$h_{\theta}(x) = p(y=1|x) = \frac{1}{1+e^{-\theta^Tx}}， p(y=0|x) = \frac{e^{-x}}{1+e^{-\theta^Tx}}$$


$$
\begin{align*}
L(\theta) & = -[ylogp + (1-y)log(1-p)] \\
                        & = -\sum_i[y_ilogp_i + (1-y_i)log(1-p_i)] \\
                        & = -\sum_i[y_ilog\frac{1}{1+e^{-\theta^Tx_i}}  + (1-y_i)log\frac{e^{-\theta^Tx_i}}{1+e^{-\theta^Tx_i}}] \\
                        & = -\sum_i[y_i\theta^Tx_i - y_ilog(1+e^{\theta^Tx_i}) - (1-y_i)log(1+e^{\theta^Tx_i})] \\
                        & = -\sum_i[y_i\theta^Tx_i - log(1+e^{\theta^Tx_i})]
\end{align*} . 

$$
利用上式子，对$$\theta_j$$求导：
$$
\begin{align*}
 \frac{\partial L}{\partial \theta_j} & = \frac{\partial (-\sum_i[y_i\theta^Tx - log(1+e^{\theta^Tx_i})])}{\partial \theta_j} \\
 																			& = -\sum_i[y_ix_j^{(i)} + \frac{x_j^{(i)}e^{\theta^Tx_i}}{1+e^{\theta^Tx_i}}] \\
 																			& = -\sum_i[y_i - h_{\theta}(x_i)]x_j^{(i)} \\
 																			& = \sum_i[h_{\theta}(x_i) - y_i]x_j^{(i)}
 																			
\end{align*}
$$
