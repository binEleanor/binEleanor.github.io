---
layout: post
title:  "浅析防止过拟合技巧"
date:   2018-08-10
excerpt: "主要介绍减少过拟合的一些技巧，如dropout， early stop， L1正则化，L2正则化等"
tag:
- 过拟合
comments: true
---

### 浅析防止过拟合技巧

#### 1. Dropout

<a href ="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">《dropout: a simple way to prevent neural networks from overfitting</a>

**dropout的核心思想**：在深度学习训练过程当中，对于神经网络单元按照一定的概率将其暂时的从原始网络中删除掉，对于随机梯度下降而言，每一个mini-batch都在训练不同的网络。

**dropout的工作流程**

对于上一层的输出，输入到当前神经网络层的时候，对当前神经网络层进行dropout的操作：

（1）随机暂时地删除掉当前网络层的某些隐藏神经单元，输入到下一层，通过反向传播等优化没有删除的神经单元的参数；

（2）恢复被删掉的神经单元，再次随机选择临时隐藏神经单元进行删除，更新没有删除的神经单元的参数；

（3）Repeat（1），（2）的操作。

**dropout的优点**

通过参数更新希望留下来的神经元能够相互协调工作，削弱减弱了所有神经元之间的共适性问题（直观理解：50个人去干一件事情的效率在某种程度上往往不如将50个人分成5个组，每个组去干某件事情的效率高），从而能够提高模型的泛化能力。

dropout往往对于大数据集的效果表现更好，对于小数据集的时候效果不够明显。

#### 2. Early stop（早停）

early stop是一种通过提前截断迭代次数来防止过拟合的方法；

**early stop 的工作流程**

（1）每一个epoch训练结束的时候，根据模型评价指标计算相应的值；

（2）每次迭代结束的时候，记录到目前为止评价指标最高的值；

（3）每次计算当前迭代评价指标得分与最高评价指标得分的差值，如果在给定阈值范围内，且连续指定epoch数目内，评价指标得分都没有上升则截断迭代次数；

#### 3. L1正则化

$$L_1$$ 正则化也被称作是Lasso Regression正则化，即为原始的目标函数值加入$$L_1$$ 范数惩罚项。
$$
L(\lambda, w) = L(w) + \lambda||w||_1
$$
$$L_1$$ 范数的定义为：对每一项的绝对值求和
$$
||w||_1 = |w_1| + |w_2| + ... + |w_n|
$$
1范数往往能够诱导稀疏解，可用于特征的选择。

直观上我们要想求解模型的稀疏解，只需要求解参数项的非零个数即可（0范数），然而求解非零个数是个NP-hard的问题，而求解1范数的解是可行的，所以通常使用1范数作为0范数的最优凸近似。

**1范数为什么能够诱导稀疏解？**

就二维空间举例而言：

$$L_1$$正则项约束的解空间是多边形，$$L_2$$正则项约束的解空间是圆形，显然多边形的解空间更容易在尖尖角处能够与凸优化目标函数等高线碰撞出稀疏解。

#### 4. L2正则化

$$L_2$$正则化也被称作是weight decay,权重衰减，让权重衰减到更小的值，从而减少模型过拟合的作用：

$$L_2$$正则化与$$L_1$$正则化的表达式类似，因为都是为目标函数加入惩罚项，不同之处在于$$L_2$$正则项加入的是2范数正则项。
$$
L(\lambda, w) = L(w) + \frac{\lambda}{2}||w||_2
$$
引入正则项之后，对参数$$w$$求导：
$$
\frac{\partial L(\lambda, w)}{\partial w} = \frac{\partial L(w)}{\partial w} + \lambda w
$$
参数的更新方式为：
$$
w := w - \eta(\frac{\partial L(w)}{\partial w} + \lambda w) = （1-\eta \lambda）w - \eta \frac{\partial L(w)}{\partial w}
$$
不引入正则项的参数更新方式为：
$$
w := w - \frac{\partial L(w)}{\partial w}
$$
两者进行对比可知：

参数的系数由$$1 \rightarrow (1-\eta \lambda)$$, $$\eta, \lambda$$均为大于0的数，所以$$1-\eta \lambda$$的值应该是小于1的，即参数更新的时候，系数部分减小的，即weight decay的由来。

**weight decay 为什么能够防止过拟合？**

1. 一般解释说，参数权值越小，其模型相对越简单，由奥卡姆剃须刀定理，越简单的模型能够模拟越复杂的模型，说明其效果更好。
2. weight decay 使得模型的参数值减小，但是其不会像$$L_1$$范数一样使得权值为0，权值接近于0，但是不会等于0，在某种程度上具有一定的抗干扰能力。
3. $$L_2$$范数的引入损失了模型的无偏性，但是增加了模型的稳定性，系数是稠密的，与此同时存在的一个缺点就是可解释性不强。



$$L_1$$正则化与$$L_2$$正则化的比较：

1. $$L_1$$范数不是连续可导函数，求解过程相对$$L_2$$范数求导复杂；
2. $$L_1$$正则化倾向于特征的选择，而$$L_2$$正则化求解出的权值是稠密的，特征选择优势不如$$L_1$$正则化；

目标函数加入正则项或者为目标函数引入约束条件，两者是等价的，都是为了约束参数项的取值范围，从而达到防止过拟合的效果。

#### 5.数据增强

数据增强是基于数据层面防止模型过拟合的基本方法



过拟合总的来说就是：

（1）减小模型的复杂度；（2）增加模型训练数据；

（3）集成学习似乎也是一种手段。







